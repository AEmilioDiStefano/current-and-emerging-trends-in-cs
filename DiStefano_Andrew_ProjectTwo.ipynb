{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treasure Hunt Game Notebook\n",
    "\n",
    "## Read and Review Your Starter Code\n",
    "The theme of this project is a popular treasure hunt game in which the player needs to find the treasure before the pirate does. While you will not be developing the entire game, you will write the part of the game that represents the intelligent agent, which is a pirate in this case. The pirate will try to find the optimal path to the treasure using deep Q-learning. \n",
    "\n",
    "You have been provided with two Python classes and this notebook to help you with this assignment. The first class, TreasureMaze.py, represents the environment, which includes a maze object defined as a matrix. The second class, GameExperience.py, stores the episodes â€“ that is, all the states that come in between the initial state and the terminal state. This is later used by the agent for learning by experience, called \"exploration\". This notebook shows how to play a game. Your task is to complete the deep Q-learning implementation for which a skeleton implementation has been provided. The code blocks you will need to complete have #TODO as a header.\n",
    "\n",
    "First, read and review the next few code and instruction blocks to understand the code that you have been given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "import matplotlib.pyplot as plt\n",
    "from TreasureMaze import TreasureMaze\n",
    "from GameExperience import GameExperience\n",
    "import math\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block contains an 8x8 matrix that will be used as a maze object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 8x8 maze with '1's for empty spaces and '0's for occupied (blocked) spaces.\n",
    "maze = np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function allows a visual representation of the maze object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function depicts the maze.\n",
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    pirate_row, pirate_col, _ = qmaze.state\n",
    "    canvas[pirate_row, pirate_col] = 0.3   # pirate cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # treasure cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pirate agent can move in four directions: left, right, up, and down. \n",
    "\n",
    "While the agent primarily learns by experience through exploitation, often, the agent can choose to explore the environment to find previously undiscovered paths. This is called \"exploration\" and is defined by epsilon. This value is typically a lower value such as 0.1, which means for every ten attempts, the agent will attempt to learn by experience nine times and will randomly explore a new path one time. You are encouraged to try various values for the exploration factor and see how the algorithm performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Exploration Factor (epsilon):\n",
    "    \n",
    "\"\"\" \n",
    "The first epoch starts with this epsilon value. Change the value of this \n",
    "variable to adjust the initial epsilon value (before decay begins). \n",
    "\"\"\"\n",
    "initial_epsilon = 0.45\n",
    "\n",
    "\"\"\" \n",
    "The final epoch starts with this epsilon value. Change the value of this \n",
    "variable to adjust the final epsilon value (after decay is complete). \n",
    "\"\"\"\n",
    "final_epsilon = 0.01\n",
    "\n",
    "\"\"\" \n",
    "This is the rate at which epsilon will decay.  Change the value of this \n",
    "variable to adjust the rate of epsilon decay. \n",
    "\"\"\"\n",
    "decay_rate = 0.98\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "This is the rate at which epsilon will decay for step decay.  Change the \n",
    "value of this variable to adjust the rate of epsilon decay \n",
    "(does variable only affects step decay). \n",
    "\"\"\"\n",
    "decay_interval = 5\n",
    "\n",
    "# This dictionary contains the agent's potential actions.\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "# The number of actions is the size of the \n",
    "# actions_dict dictionary.\n",
    "num_actions = len(actions_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample code block and output below show creating a maze object and performing one action (DOWN), which returns the reward. The resulting updated environment is visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward= -0.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b0bedb2fc8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAFtElEQVR4nO3dMWpUexjG4W8ugoUJKLmQxlIY+5kFTDpX4gpO5w5kUguuwFZcwJkFzBSW6SwCEkgjamVxbnEVFBJz5yb5Z97j88BUEd6TGX6YNPkmwzAUsPv+uusHAP4bsUIIsUIIsUIIsUIIsUKIe9v84729veHg4OC2nuUX3759q48fPzbZevr0aT148KDJ1tevX0e51XpvrFsfPnyo8/PzyUVf2yrWg4ODevHixc081RU+f/5cXdc12Xr16lUtFosmW6vVapRbrffGujWfzy/9mh+DIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIcRWf+T706dP9e7du9t6ll+0/OPU3IzNZlNHR0dNtvq+b7KzSyZXXT6fTCbPq+p5VdWjR49mL1++bPFctb+/X6enp022ptNp7e3tNdn68uXLKLeqqs7Oznxm19R1Xa3X6/93PmMYhtdV9bqq6uHDh8Pbt29v+PEutlgsmp3P6Pt+lKcYWp/POD4+9pndIr+zQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQgixQoitzmc8efKk2fmM1WpVV10LuMmtsZpMLvzj7rei7/tmn9nx8XGzUx3L5XIn/sj3VuczDg8PZ2/evGnxXKM9M9F66+TkpMlWVduTFi1PdTx+/LgODw+bbP3ufEYNw/CfX7PZbGil73tbN7BVVc1eLb+35XLZ7PtaLpfNvq/vjV3Yn99ZIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYTzGXew1eqkRcuzD1Xj/sxabTmfsWNbNcKzDz++N1vX43wGjIBYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYIYRYq2qz2dRkMmny2mw2W11BuM5rNpvd9VvLDXLrpqrOzs7q9PS0yVbL+zMt38PWe2PdcuvmCsvlcpT3Z1q+h633xrrl1g2MgFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFghhFirajabNT1p0fJUR0utz5CMdesyzmfcwdbJyUmTrZanOqranyEZ41bXdTUMg/MZu7JVIzzVMQztz5CMcevfJJ3PgGhihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRD37voBGI8fZ0haWK1Wo9yaz+eXfs35jDvYGuv5jDF/Zq22uq6r9XrtfMaubNVIz2eM+TNr5XtjzmdAMrFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCOczRr7V6lRHVdV0Oh3t+3j//v0mW13X1fv37y88n3FlrD+bz+fDer2+sQf7ndVqVYvFwtY1t46OjppsVVX1fT/a93E6nTbZevbs2aWx+jEYQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQogVQmx1PqOqplXV6h7D31V1bitmq/XeWLemwzDsX/SFrc5ntDSZTNbDMMxtZWy13vsTt/wYDCHECiF2OdbXtqK2Wu/9cVs7+zsr8Ktd/p8V+IlYIYRYIYRYIYRYIcQ/8eViVeWzLxQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze = TreasureMaze(maze)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function simulates a full game based on the provided trained model. The other parameters include the TreasureMaze object and the starting position of the pirate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, pirate_cell):\n",
    "    qmaze.reset(pirate_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        \n",
    "        # Get the next action.\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # Apply the action, get the rewards and the new state.\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function helps you to determine whether the pirate can win any game at all. If your maze is not well designed, the pirate may not win any game at all. In this case, your training would not yield any result. The provided maze in this notebook ensures that there is a path to win and you can run this method to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for completion of the maze:\n",
    "\n",
    "# This function takes in our model and our maze as parameters.\n",
    "def completion_check(model, qmaze):\n",
    "    \n",
    "    # For every free cell,\n",
    "    for cell in qmaze.free_cells:\n",
    "        \n",
    "        # If there are no valid actions, return False.\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        \n",
    "        # If the game is over, return False.\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    \n",
    "    # Otherwise, return True.\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code you have been given in this block will build the neural network model. Review the code and note the number of layers, as well as the activation, optimizer, and loss functions that are used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function builds our maze.\n",
    "def build_model(maze):\n",
    "    \n",
    "    # Make our model sequential.\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add a dense layer.\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    \n",
    "    # Add a Parametric Rectified Linear Unit activation layer.\n",
    "    model.add(PReLU())\n",
    "    \n",
    "    # Add another dense layer.\n",
    "    model.add(Dense(maze.size))\n",
    "    \n",
    "    # Add another Parametric Rectified Linear Unit activation layer.\n",
    "    model.add(PReLU())\n",
    "    \n",
    "    # Add a final dense layer.\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDED EPSILON DECAY FUNCTIONS:\n",
    " \n",
    " These functions offer three choices for the decay of epsillon over time:\n",
    "- Linear Decay\n",
    "- Exponential Decay\n",
    "- Step (or discrete interval) decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decay epsilon linearly from an initial value to a final value over a specified number of epochs:\n",
    "\n",
    "# This function takes parameters for the initial epsilon value, the final epsilon value, \n",
    "# the number of total epochs, and the current epoch number.\n",
    "def linear_epsilon_decay(initial_epsilon, final_epsilon, total_epochs, current_epoch):\n",
    "    \n",
    "    # Try to calculate the new epsilon value with linear decay.\n",
    "    try:\n",
    "        return initial_epsilon - (current_epoch / total_epochs) * (initial_epsilon - final_epsilon)\n",
    "    \n",
    "    # If calculating the new epsilon value throws a TypeError or ValueError, inform the user \n",
    "    # and recommend that they check their parameter, variable, and value data types.\n",
    "    except (TypeError, ValueError) as e:\n",
    "        print(\"Something went wrong while attempting to calculate the linear epsilon decay. \", e)\n",
    "        print(\"One or more variable or function parameters is set to an incompatible data type, or the value itself is not of the appropriate type. \", e)\n",
    "        print(\"Check that your variables and parameters are of the correct data type. \", e)\n",
    "    \n",
    "    # If the calculation of the new epsilon value throws another type of error, then \n",
    "    # infom the user that something went wrong while attempting to calculate the \n",
    "    # linear epsilon decay.\n",
    "    except: \n",
    "        print(\"Something went wrong while attempting to calculate the linear epsilon decay. \") \n",
    "        \n",
    "\n",
    "# Decay epsilon exponentially each epoch:\n",
    "# This function takes parameters for the initial epsilon value, the final epsilon value, \n",
    "# and the current epoch.\n",
    "def exponential_epsilon_decay(initial_epsilon, decay_rate, current_epoch):\n",
    "    \n",
    "    # Try to calculate the new epsilon value with exponential decay.\n",
    "    try:\n",
    "        return initial_epsilon * (decay_rate ** current_epoch)\n",
    "    \n",
    "    # If calculating the new epsilon value throws a TypeError or ValueError, inform the user \n",
    "    # and recommend that they check their parameter, variable, and data types.\n",
    "    except (TypeError, ValueError) as e:\n",
    "        print(\"Something went wrong while attempting to calculate the exponential epsilon decay. \", e)\n",
    "        print(\"One or more variable or function parameters is set to an incompatible data type, or the value itself is not of the appropriate type. \", e)\n",
    "        print(\"Check that your variables and parameters are of the correct data type. \", e)\n",
    "    \n",
    "    # If the calculation of the new epsilon value throws another type of error, then \n",
    "    # infom the user that something went wrong while attempting to calculate the \n",
    "    # exponential epsilon decay.\n",
    "    except: \n",
    "        print(\"Something went wrong while attempting to calculate the exponential epsilon decay. \") \n",
    "\n",
    "# Drop epsillon at a specific interval of epochs (decay_interval):\n",
    "# This function takes the initial epsilon value, the decay rate, the interval at which \n",
    "# epsilon will be decayed (decay_interval), and the current epoch as paramerters.\n",
    "def step_epsilon_decay(initial_epsilon, decay_rate, decay_interval, current_epoch):\n",
    "    \n",
    "    # Try to calculate the new epsilon value with step decay.\n",
    "    try: \n",
    "        return initial_epsilon * (decay_rate ** (current_epoch // decay_interval))\n",
    "    \n",
    "    # If calculating the new epsilon value throws a TypeError or ValueError, inform the user \n",
    "    # and recommend that they check their parameter and variable data types and values.\n",
    "    except (TypeError, ValueError) as e:\n",
    "        print(\"Something went wrong while attempting to calculate the step epsilon decay. \", e)\n",
    "        print(\"One or more variable or function parameters is set to an incompatible data type, or the value itself is not of the appropriate type. \", e)\n",
    "        print(\"Check that your variables and parameters are of the correct data type. \", e)\n",
    "    \n",
    "   # If the calculation of the new epsilon value throws another type of error, then \n",
    "    # infom the user that something went wrong while attempting to calculate step epsilon decay.\n",
    "    except: \n",
    "        print(\"Something went wrong while attempting to calculate the step epsilon decay. \") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDED DATA SIZE ADJUSTMENT FUNCTION:\n",
    "\n",
    "This function adjusts the data size parameter dynamically based on the epoch number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the data size by periodically decreasing the data size.\n",
    "\n",
    "# This function takes in the current data size and the epoch number as functions.\n",
    "def adjust_data_size(data_size, epoch):\n",
    "    \n",
    "    # Initialize a new_data_size variable.\n",
    "    new_data_size = data_size\n",
    "    \n",
    "    \"\"\" Change the value of this variable to adjust the number of steps at which the data size is decreased. \"\"\"\n",
    "    number_of_steps = 7\n",
    "    \n",
    "    # If the current epoch number is a multiple of the value of the number_of_steps variable,\n",
    "    # (if there is no remainder when it is divided by number_of_steps)\n",
    "    # and the current epoch number is not 0,\n",
    "    if epoch % number_of_steps == 0 and epoch != 0:\n",
    "        \n",
    "        # then, if the data size is greater than 1,\n",
    "        if data_size > 1:\n",
    "            \n",
    "            \"\"\" Change the value of this variable to decrease the data size by the desired amount. \"\"\"\n",
    "            subtract_by = 2\n",
    "            \n",
    "            # set the new data size to the original data size minus the value of the subtract_by variable. \n",
    "            new_data_size = data_size - subtract_by\n",
    "    \n",
    "    # Otherwise, if the current epoch number is not a multiple of 10,\n",
    "    else:\n",
    "        \n",
    "        # then leave the data size unchanged.\n",
    "        new_data_size = data_size\n",
    "        \n",
    "    # Return the new data size, whether changed or unchanged.\n",
    "    return new_data_size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION FOR PRINTING AN EPOCH: \n",
    "\n",
    "This function has been taken out of the qtrain function to modularize the function to improve readability, reusability, and maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function prints the results of an epoch.  The function takes the current epoch number\n",
    "# (epoch), the number of epochs (n_epochs), data size, loss, number of episodes (n_episodes), \n",
    "# win history, and win rate as parameters. \n",
    "def print_epoch(epoch, n_epoch, epsilon, data_size, loss, n_episodes, win_history, win_rate):\n",
    "    # start time\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    # Try printing the results of an epoch.\n",
    "    try:\n",
    "        \n",
    "        #Print the epoch, loss, episodes, win count, and win rate for each epoch\n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Epsilon: {:.4f} | Data Size: {} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {} \\n\"\n",
    "        print(template.format(epoch, n_epoch-1, epsilon, data_size, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "    \n",
    "    # If the printing of the epoch results throws a TypeError or a ValueError, inform the user\n",
    "    # and recommend that they check their parameter and variable data types and values.\n",
    "    except (TypeError, ValueError) as e:\n",
    "        print(\"Something went wrong while attempting to print the epoch results.\", e)\n",
    "        print(\"One or more variable or function parameters is set to an incompatible data type, or the value itself is not of the appropriate type. \", e)\n",
    "        print(\"Check that your variables and parameters are of the correct data type. \", e)\n",
    "    \n",
    "    # If the printing of epoch results throws another type of error,  inform \n",
    "    # the user and recommend that something went wrong while attempting to \n",
    "    # print the epoch results.\n",
    "    except:\n",
    "        print(\"Something went wrong while attempting to print the epoch results. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTION FOR THE TRAINING LOOP:\n",
    "\n",
    "This function has been taken out of the qtrain function to modularize the function to improve readability, reusability, and maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function executes our training loop.  The function takes the number of epochs (n_epoch), maximum \n",
    "# memory, data size, start time, the maze state (qmaze), especience, win history, and win rate parameters.\n",
    "def training_loop(n_epoch, max_memory, data_size, start_time, qmaze, experience, win_history, hsize, win_rate):\n",
    "    # For each epoch:\n",
    "    for epoch in range(n_epoch):\n",
    "    \n",
    "        # Set the value of the boolean game_over variable to False initially.\n",
    "        game_over = False\n",
    "        \n",
    "        # Set the number of episodes to zero initially.\n",
    "        n_episodes = 0\n",
    "        \n",
    "        # Set the loss to zero initially.\n",
    "        loss = 0.0\n",
    "        \n",
    "        # Randomly select a free cell for the agent to move to.\n",
    "        agent_cell = random.choice(qmaze.free_cells)\n",
    "        \n",
    "        # Reset the maze with agent set to the previous position.\n",
    "        qmaze.reset(agent_cell)\n",
    "\n",
    "        # Use the 'observe()' method to set the envstate variable's value to the new state.\n",
    "        envstate = qmaze.observe()\n",
    "        \n",
    "        # Use one of the three epsilon decay functions to decay epsilon over time:\n",
    "        \"\"\"\n",
    "        The three lines of code below this docstring, including those which \n",
    "        are commented out, can be used to call each of our epsilon decay \n",
    "        functions.  Uncomment the line of code which represents the appropriate \n",
    "        function that uses the type of decay which you would like to use.  \n",
    "        Only one of these three lines should be uncommented.\n",
    "        \"\"\"\n",
    "        \"\"\" Uncomment this line for linear decay. \"\"\"\n",
    "        #epsilon = linear_epsilon_decay(initial_epsilon, final_epsilon, n_epoch, epoch)\n",
    "        \n",
    "        \"\"\" Uncomment this line for exponential decay. \"\"\"\n",
    "        epsilon = exponential_epsilon_decay(initial_epsilon, decay_rate, epoch)\n",
    "        \n",
    "        \"\"\" Uncomment this line for step (discrete interval) decay. \"\"\"\n",
    "        #epsilon = step_epsilon_decay(initial_epsilon, decay_rate, decay_interval, epoch)\n",
    "        \n",
    "        # Use the adjust_data_size() function to adjust the data size.\n",
    "        # Pass in the data_size variable and the epoch variable as parameters.\n",
    "        data_size = adjust_data_size(data_size, epoch)\n",
    "        \n",
    "        # While state is not game over:\n",
    "        while not game_over:\n",
    "            \n",
    "            # Get the possible actions (valid actions) which the agent can take\n",
    "            # in the current state.\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            \n",
    "            # If the action is not valid, then break.\n",
    "            if not valid_actions:\n",
    "                break\n",
    "            \n",
    "            # Set the previous endstate to endstate.\n",
    "            prev_envstate = envstate\n",
    "            \n",
    "             # Randomly choose an action (left, right, up, down) either by exploration or by exploitation.\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(envstate))\n",
    "            \n",
    "             # The agent acts in consideration of the current game and the reward specifications.\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            \n",
    "            # If the agent wins, add one to the number of wins and set 'game_over' to True.\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "                \n",
    "            # If the agent loses, do not append another win, and set 'game_over' to True.\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "                \n",
    "            # If the agent has not won or lost, the game is not over and 'game_over' is set to False.\n",
    "            else: \n",
    "                game_over = False\n",
    "            \n",
    "            # Put the values of these variablestogether into an 'episode'.\n",
    "            episode = [prev_envstate, action, reward, envstate,game_status]\n",
    "            \n",
    "            # Add one to the number of episodes.\n",
    "            n_episodes += 1\n",
    "            \n",
    "            # Store episode in an experience replay object.\n",
    "            experience.remember(episode)\n",
    "            \n",
    "            # Call GameExperience.get_data to retrieve training data (input and target).\n",
    "            inputs,targets = experience.get_data(data_size=data_size)\n",
    "            \n",
    "            # Train the neural network model.\n",
    "            history = model.fit(inputs, targets, epochs=12, batch_size=32, verbose=0)\n",
    "\n",
    "            # Evaluate the loss. \n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "            \n",
    "        # If the agent wins every game in an epoch, then the win rate is 1.0. \n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "\n",
    "        # Print the current epoch using the print_epoch() function, passing in the current\n",
    "        # epoch number (epoch), number of epochs (n_epoch), exploration factor (epsilon), \n",
    "        # data size, loss, number of epusides (n_episodes), win history, and win rate.\n",
    "        print_epoch(epoch, n_epoch, epsilon, data_size, loss, n_episodes, win_history, win_rate)\n",
    "        \n",
    "        \n",
    "        # Implement an early stopping technique to prevent overfitting \n",
    "        # and improve learning efficiency:\n",
    "        \n",
    "        # If the win rate reaches 100%,\n",
    "        if win_rate == 1.0:\n",
    "            \n",
    "            # then print the epoch at which a win rate of 100%,\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "                        \n",
    "            # and exit the program.\n",
    "            break\n",
    "    \n",
    "    # Try to print the final results after training.\n",
    "    try:\n",
    "        # Print the final results\n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        seconds = dt.total_seconds()\n",
    "        t = format_time(seconds)\n",
    "        print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "        return seconds\n",
    "    \n",
    "    # If the printing of final results throws a TypeError or a ValueError, inform the user\n",
    "    # and recommend that they check their parameter, variable, and value data types.\n",
    "    except (TypeError, ValueError) as e:\n",
    "        print(\"Something went wrong while attempting to print the final results. \", e)\n",
    "        print(\"One or more variable or function parameters is set to an incompatible data type, or the value itself is not of the appropriate type. \", e)\n",
    "        print(\"Check that your variables and parameters are of the correct data type. \", e)\n",
    "    \n",
    "    # If the printing of final results throws another type of error, then \n",
    "    # infom the user that something went wrong while attempting to print\n",
    "    # the final results.\n",
    "    except: \n",
    "        print(\"Something went wrong while attempting to print the final results. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Training Algorithm Code Block\n",
    "\n",
    "This is the deep Q-learning implementation. The goal of our deep Q-learning implementation is to find the best possible navigation sequence that results in reaching the treasure cell while maximizing the reward. \n",
    "\n",
    "You will need to complete the section starting with #pseudocode. The pseudocode has been included for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function trains our model.  The function takes in the model and maze as parameters, and \n",
    "# collects any extra keyword arguments passed to the function into opt. \n",
    "def qtrain(model, maze, **opt):\n",
    "    \n",
    "    # number of epochs\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "\n",
    "    # maximum memory to store episodes\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "\n",
    "    # maximum data size for training\n",
    "    data_size = opt.get('data_size', 50)\n",
    "\n",
    "    # start time\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Construct the game environment from a numpy array: maze.\n",
    "    qmaze = TreasureMaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = GameExperience(model, max_memory=max_memory)\n",
    "    \n",
    "    # history of win/lose game\n",
    "    win_history = []   \n",
    "    \n",
    "    # history window size\n",
    "    hsize = qmaze.maze.size//2   \n",
    "    win_rate = 0.0\n",
    "\n",
    "    training_loop(n_epoch, max_memory, data_size, start_time, qmaze, experience, win_history, hsize, win_rate)\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Your Model\n",
    "\n",
    "Now we will start testing the deep Q-learning implementation. To begin, select **Cell**, then **Run All** from the menu bar. This will run your notebook. As it runs, you should see output begin to appear beneath the next few cells. The code below creates an instance of TreasureMaze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b0bee23fc8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze = TreasureMaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code block, you will build your model and train it using deep Q-learning. Note: This step takes several minutes to fully run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/14999 | Epsilon: 0.4500 | Data Size: 32 | Loss: 0.0109 | Episodes: 163 | Win count: 1 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 001/14999 | Epsilon: 0.4410 | Data Size: 32 | Loss: 0.0014 | Episodes: 10 | Win count: 2 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 002/14999 | Epsilon: 0.4322 | Data Size: 32 | Loss: 0.0015 | Episodes: 140 | Win count: 3 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 003/14999 | Epsilon: 0.4235 | Data Size: 32 | Loss: 0.0012 | Episodes: 83 | Win count: 4 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 004/14999 | Epsilon: 0.4151 | Data Size: 32 | Loss: 0.0006 | Episodes: 10 | Win count: 5 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 005/14999 | Epsilon: 0.4068 | Data Size: 32 | Loss: 0.0009 | Episodes: 1 | Win count: 6 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 006/14999 | Epsilon: 0.3986 | Data Size: 32 | Loss: 0.0022 | Episodes: 92 | Win count: 7 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 007/14999 | Epsilon: 0.3907 | Data Size: 30 | Loss: 0.0008 | Episodes: 17 | Win count: 8 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 008/14999 | Epsilon: 0.3828 | Data Size: 30 | Loss: 0.0011 | Episodes: 41 | Win count: 9 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 009/14999 | Epsilon: 0.3752 | Data Size: 30 | Loss: 0.0008 | Episodes: 35 | Win count: 10 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 010/14999 | Epsilon: 0.3677 | Data Size: 30 | Loss: 0.0011 | Episodes: 84 | Win count: 11 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 011/14999 | Epsilon: 0.3603 | Data Size: 30 | Loss: 0.0012 | Episodes: 154 | Win count: 11 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 012/14999 | Epsilon: 0.3531 | Data Size: 30 | Loss: 0.0012 | Episodes: 78 | Win count: 12 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 013/14999 | Epsilon: 0.3461 | Data Size: 30 | Loss: 0.0015 | Episodes: 82 | Win count: 13 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 014/14999 | Epsilon: 0.3391 | Data Size: 28 | Loss: 0.0021 | Episodes: 9 | Win count: 14 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 015/14999 | Epsilon: 0.3324 | Data Size: 28 | Loss: 0.0014 | Episodes: 9 | Win count: 15 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 016/14999 | Epsilon: 0.3257 | Data Size: 28 | Loss: 0.0013 | Episodes: 75 | Win count: 16 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 017/14999 | Epsilon: 0.3192 | Data Size: 28 | Loss: 0.0012 | Episodes: 28 | Win count: 17 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 018/14999 | Epsilon: 0.3128 | Data Size: 28 | Loss: 0.0009 | Episodes: 81 | Win count: 18 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 019/14999 | Epsilon: 0.3066 | Data Size: 28 | Loss: 0.0012 | Episodes: 47 | Win count: 19 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 020/14999 | Epsilon: 0.3004 | Data Size: 28 | Loss: 0.0011 | Episodes: 41 | Win count: 20 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 021/14999 | Epsilon: 0.2944 | Data Size: 26 | Loss: 0.0012 | Episodes: 13 | Win count: 21 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 022/14999 | Epsilon: 0.2885 | Data Size: 26 | Loss: 0.0016 | Episodes: 31 | Win count: 22 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 023/14999 | Epsilon: 0.2828 | Data Size: 26 | Loss: 0.0015 | Episodes: 15 | Win count: 23 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 024/14999 | Epsilon: 0.2771 | Data Size: 26 | Loss: 0.0013 | Episodes: 77 | Win count: 24 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 025/14999 | Epsilon: 0.2716 | Data Size: 26 | Loss: 0.0014 | Episodes: 26 | Win count: 25 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 026/14999 | Epsilon: 0.2661 | Data Size: 26 | Loss: 0.0014 | Episodes: 61 | Win count: 26 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 027/14999 | Epsilon: 0.2608 | Data Size: 26 | Loss: 0.0017 | Episodes: 54 | Win count: 27 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 028/14999 | Epsilon: 0.2556 | Data Size: 24 | Loss: 0.0013 | Episodes: 36 | Win count: 28 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 029/14999 | Epsilon: 0.2505 | Data Size: 24 | Loss: 0.0012 | Episodes: 35 | Win count: 29 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 030/14999 | Epsilon: 0.2455 | Data Size: 24 | Loss: 0.0007 | Episodes: 11 | Win count: 30 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 031/14999 | Epsilon: 0.2406 | Data Size: 24 | Loss: 0.0021 | Episodes: 36 | Win count: 31 | Win rate: 0.000 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 032/14999 | Epsilon: 0.2357 | Data Size: 24 | Loss: 0.0021 | Episodes: 34 | Win count: 32 | Win rate: 0.969 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 033/14999 | Epsilon: 0.2310 | Data Size: 24 | Loss: 0.0015 | Episodes: 1 | Win count: 33 | Win rate: 0.969 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 034/14999 | Epsilon: 0.2264 | Data Size: 24 | Loss: 0.0009 | Episodes: 21 | Win count: 34 | Win rate: 0.969 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 035/14999 | Epsilon: 0.2219 | Data Size: 22 | Loss: 0.0011 | Episodes: 68 | Win count: 35 | Win rate: 0.969 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 036/14999 | Epsilon: 0.2174 | Data Size: 22 | Loss: 0.0009 | Episodes: 68 | Win count: 36 | Win rate: 0.969 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 037/14999 | Epsilon: 0.2131 | Data Size: 22 | Loss: 0.0012 | Episodes: 84 | Win count: 37 | Win rate: 0.969 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 038/14999 | Epsilon: 0.2088 | Data Size: 22 | Loss: 0.0007 | Episodes: 18 | Win count: 38 | Win rate: 0.969 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 039/14999 | Epsilon: 0.2047 | Data Size: 22 | Loss: 0.0008 | Episodes: 16 | Win count: 39 | Win rate: 0.969 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 040/14999 | Epsilon: 0.2006 | Data Size: 22 | Loss: 0.0010 | Episodes: 7 | Win count: 40 | Win rate: 0.969 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 041/14999 | Epsilon: 0.1966 | Data Size: 22 | Loss: 0.0006 | Episodes: 15 | Win count: 41 | Win rate: 0.969 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 042/14999 | Epsilon: 0.1926 | Data Size: 20 | Loss: 0.0010 | Episodes: 29 | Win count: 42 | Win rate: 0.969 | time: 0.0 seconds \n",
      "\n",
      "Epoch: 043/14999 | Epsilon: 0.1888 | Data Size: 20 | Loss: 0.0014 | Episodes: 42 | Win count: 43 | Win rate: 1.000 | time: 0.0 seconds \n",
      "\n",
      "Reached 100% win rate at epoch: 43\n",
      "n_epoch: 43, max_mem: 512, data: 20, time: 8.17 minutes\n"
     ]
    }
   ],
   "source": [
    "model = build_model(maze)\n",
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will check to see if the model passes the completion check. Note: This could take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b0bed55a08>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "completion_check(model, qmaze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will test your model for one game. It will start the pirate at the top-left corner and run play_game. The agent should find a path from the starting position to the target (treasure). The treasure is located in the bottom-right corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b0bee5b708>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pirate_start = (0, 0)\n",
    "play_game(model, qmaze, pirate_start)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Submit Your Work\n",
    "After you have finished creating the code for your notebook, save your work. Make sure that your notebook contains your name in the filename (e.g. Doe_Jane_ProjectTwo.ipynb). This will help your instructor access and grade your work easily. Download a copy of your IPYNB file and submit it to Brightspace. Refer to the Jupyter Notebook in Apporto Tutorial if you need help with these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
